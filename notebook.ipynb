{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Environment setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tCUDA and NVIDIA Drivers: CUDA 12.3 with Driver 545.23.08\n",
    "-\tUbuntu 22.04\n",
    "-\t[NVIDIA-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) version 1.15.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Clone the NeMo Curator repository and install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA/NeMo-Curator.git\n",
    "cd NeMo-Curator\n",
    "pip install --extra-index-url https://pypi.nvidia.com \".[cuda12x]\"\n",
    "pip install datasets\n",
    "pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a Dask environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_curator\n",
    "from dask.distributed import Client, LocalCluster\n",
    "# Start a Dask cluster with 12 workers, each limited at 64GB of memory. You might need to adjust these numbers according to your computing resources\n",
    "cluster = LocalCluster(n_workers=12, processes=True, memory_limit= '64GB')\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is accessed and downloaded using the Hugging Face Hub, with additional steps required for OSCAR due to its access restrictions. For OSCAR, you need to accept the conditions on the [dataset page](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301) and use a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) for downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and Convert Datasets to Parquet**\n",
    "\n",
    "The conversion of dataset into Parquet format facilitates efficient handling and processing of large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from datasets import DownloadConfig \n",
    "\n",
    "data_dir = \"./datasets/\"\n",
    "download_config = DownloadConfig(num_proc=4)\n",
    "\n",
    "# Load and save Vietnamese Wikipedia dataset\n",
    "ds = load_hf_dataset(\"wikimedia/wikipedia\", \"20231101.vi\")\n",
    "ds['train'].to_parquet(os.path.join(data_dir, \"wiki_vi_231101.parquet\"))\n",
    "\n",
    "# Load and save Vietnamese news corpus\n",
    "ds = load_hf_dataset(\"jetaudio/binhvq_news\")\n",
    "ds['train'].to_parquet(os.path.join(data_dir, \"binhvq_news_train.parquet\"))\n",
    "\n",
    "# Load and save OSCAR dataset\n",
    "ds = load_hf_dataset(\"oscar-corpus/OSCAR-2301\", language='vi', token=True, download_config=download_config, trust_remote_code=True)\n",
    "ds['train'].to_parquet(os.path.join(data_dir, 'oscar_vi.parquet'))\n",
    "\n",
    "# Load and save C4 dataset\n",
    "ds = load_hf_dataset(\"allenai/c4\", data_files='multilingual/c4-vi.*.json.gz', download_config=download_config, trust_remote_code=True)\n",
    "ds['train'].to_parquet(os.path.join(data_dir, \"c4_vi.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine and Standardize Format**\n",
    "\n",
    "We then combine them into a single dataset, keeping only the 'text' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# Combine datasets and standardize format\n",
    "datasets = [os.path.join(data_dir, file) for file in [\"wiki_vi_231101.parquet\", \"c4_vi.parquet\", 'oscar_vi.parquet', \"binhvq_news_train.parquet\"]]\n",
    "\n",
    "data_files = {\"train\": datasets[0]}\n",
    "ds = load_hf_dataset(\"parquet\", data_files=data_files)\n",
    "ds = ds['train'].remove_columns([col for col in ds['train'].column_names if col != 'text'])\n",
    "\n",
    "for d in datasets[1:]:\n",
    "    ds_ = load_hf_dataset(\"parquet\", data_files={\"train\": d})\n",
    "    ds_ = ds_['train'].remove_columns([col for col in ds_['train'].column_names if col != 'text'])\n",
    "    ds = concatenate_datasets([ds, ds_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shard the Combined Dataset**\n",
    "\n",
    "The combined dataset is then sharded into smaller chunks. Sharding is performed to distribute the data evenly across multiple workers in the Dask cluster, facilitating efficient parallel processing during the data curation stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for raw data\n",
    "raw_data_directory = os.path.join(data_dir, \"raw\")\n",
    "\n",
    "# Shard the dataset\n",
    "num_shards = 256\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds.shard(index=shard_idx, num_shards=num_shards)\n",
    "    shard.to_parquet(os.path.join(raw_data_directory, f\"{shard_idx}.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import Modify\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.distributed_utils import read_data, write_to_disk\n",
    "from nemo_curator.utils.file_utils import get_all_files_paths_under\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "\n",
    "# Define paths for Unicode formatted data\n",
    "unicode_formatted_output_path = os.path.join(data_dir, \"formatted\")\n",
    "\n",
    "def load_dataset(input_data_dir, file_type='parquet'):\n",
    "    files = list(get_all_files_paths_under(input_data_dir))\n",
    "    raw_data = read_data(files, file_type=file_type, backend=\"pandas\", add_filename=True)\n",
    "    dataset = DocumentDataset(raw_data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Load the raw data\n",
    "raw_data = load_dataset(raw_data_directory, file_type='parquet')\n",
    "\n",
    "# Initialize the Unicode reformatter\n",
    "cleaner = Modify(UnicodeReformatter())\n",
    "\n",
    "# Apply Unicode reformatting\n",
    "cleaned_data = cleaner(raw_data)\n",
    "\n",
    "# Save the cleaned data to disk\n",
    "write_to_disk(cleaned_data.df, unicode_formatted_output_path, write_to_filename=True, output_type='parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Custom IDs to Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import AddId\n",
    "\n",
    "# Define paths for input data and output with added IDs\n",
    "add_id_input_data_dir = unicode_formatted_output_path\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id\")\n",
    "add_ID_id_prefix = \"VI_\"\n",
    "\n",
    "# Load the formatted dataset\n",
    "dataset = load_dataset(add_id_input_data_dir, file_type='parquet')\n",
    "\n",
    "# Initialize the AddId class with a specified prefix and start index\n",
    "add_id = AddId(id_field='id', id_prefix=add_ID_id_prefix, start_index=0)\n",
    "\n",
    "# Apply the ID addition to the dataset\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "# Save the dataset with added IDs to disk\n",
    "write_to_disk(id_dataset.df, output_file_dir=added_id_output_path, write_to_filename=True, output_type='parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator import AddId\n",
    "\n",
    "# Define paths for input data and output with added IDs\n",
    "add_id_input_data_dir = unicode_formatted_output_path\n",
    "added_id_output_path = os.path.join(data_dir, \"add_id\")\n",
    "add_ID_id_prefix = \"VI_\"\n",
    "\n",
    "# Load the formatted dataset\n",
    "dataset = load_dataset(add_id_input_data_dir, file_type='parquet')\n",
    "\n",
    "# Initialize the AddId class with a specified prefix and start index\n",
    "add_id = AddId(id_field='id', id_prefix=add_ID_id_prefix, start_index=0)\n",
    "\n",
    "# Apply the ID addition to the dataset\n",
    "id_dataset = add_id(dataset)\n",
    "\n",
    "# Save the dataset with added IDs to disk\n",
    "write_to_disk(id_dataset.df, output_file_dir=added_id_output_path, write_to_filename=True, output_type='parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "import warnings\n",
    "\n",
    "# Define paths for input data and output data after heuristic filtering\n",
    "HF_input_data_dir = deduped_output_dir\n",
    "HF_output_path = os.path.join(data_dir, 'heuristic_filtering')\n",
    "\n",
    "# Create a directory for the configuration file if it doesn't exist\n",
    "os.makedirs('config', exist_ok=True)\n",
    "# Download the YAML configuration file for heuristic filtering\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml -O ./config/heuristic_filter_non-en.yaml\n",
    "\n",
    "# Specify the path to the configuration file\n",
    "filter_config_file = './config/heuristic_filter_non-en.yaml'\n",
    "os.makedirs(HF_output_path, exist_ok=True)\n",
    "\n",
    "# Load the filters from the YAML configuration file\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = DocumentDataset.read_parquet(HF_input_data_dir, backend='pandas')\n",
    "\n",
    "# Suppress specific warnings during filtering\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Apply the heuristic filters to the dataset\n",
    "    result_data = filter_pipeline(dataset)\n",
    " \n",
    "    # Save the filtered dataset to disk\n",
    "    result_data.to_parquet(HF_output_path, write_to_filename=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-based quality filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier-based filtering uses a trained classifier model to sort content as high or low quality, offering a smarter and more flexible way to handle diverse datasets that simple rules might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data for Training Classifier**\n",
    "\n",
    "To train a quality classifier, we need representative samples of both high-quality and low-quality content. For high-quality data, we use articles from Wikipedia's Vietnamese edition, which are generally well-structured and reliable. The low-quality samples come from unfiltered crawled Vietnamese news corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.config_utils import build_filter_pipeline\n",
    "import warnings\n",
    "\n",
    "# Define paths for input data and output data after heuristic filtering\n",
    "HF_input_data_dir = deduped_output_dir\n",
    "HF_output_path = os.path.join(data_dir, 'heuristic_filtering')\n",
    "\n",
    "# Create a directory for the configuration file if it doesn't exist\n",
    "os.makedirs('config', exist_ok=True)\n",
    "# Download the YAML configuration file for heuristic filtering\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/config/heuristic_filter_non-en.yaml -O ./config/heuristic_filter_non-en.yaml\n",
    "\n",
    "# Specify the path to the configuration file\n",
    "filter_config_file = './config/heuristic_filter_non-en.yaml'\n",
    "os.makedirs(HF_output_path, exist_ok=True)\n",
    "\n",
    "# Load the filters from the YAML configuration file\n",
    "filter_pipeline = build_filter_pipeline(filter_config_file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = DocumentDataset.read_parquet(HF_input_data_dir, backend='pandas')\n",
    "\n",
    "# Suppress specific warnings during filtering\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Apply the heuristic filters to the dataset\n",
    "    result_data = filter_pipeline(dataset)\n",
    " \n",
    "    # Save the filtered dataset to disk\n",
    "    result_data.to_parquet(HF_output_path, write_to_filename=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training classifier**\n",
    "\n",
    "The classifier is trained using FastText, which offers an efficient and effective method for text classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.modifiers import FastTextLabelModifier\n",
    "import fasttext\n",
    "import random\n",
    "\n",
    "# Function to create labeled samples\n",
    "def create_samples(data_path, label, num_samples):\n",
    "    raw_dataset = DocumentDataset.read_parquet(data_path, backend='pandas')\n",
    "    label_quality = Modify(FastTextLabelModifier(label))\n",
    "    labeled_dataset = label_quality(raw_dataset)\n",
    "    labeled_samples = labeled_dataset.df.sample(frac=num_samples / len(labeled_dataset.df))\n",
    "    return labeled_samples[\"text\"].compute().values.tolist()\n",
    "\n",
    "# Prepare training data\n",
    "low_quality_samples = create_samples(lq_samples_path, \"__label__lq\", 100000)\n",
    "high_quality_samples = create_samples(hq_samples_path, \"__label__hq\", 100000)\n",
    "train_samples = low_quality_samples + high_quality_samples\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "# Save training data to a file\n",
    "train_file = \"./cf_model_fasttext.train\"\n",
    "with open(train_file, \"w\") as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(sample + \"\\n\")\n",
    "\n",
    "# Train the FastText classifier\n",
    "model = fasttext.train_supervised(input=train_file, lr=0.01, dim=100, epoch=5, wordNgrams=2)\n",
    "model_path = \"./cf_model_fasttext_model.bin\"\n",
    "model.save_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify and filter the dataset**\n",
    "\n",
    "Once trained, the classifier is used to filter the dataset, categorizing documents into high and low quality based on the learned distinctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.filters import FastTextQualityFilter\n",
    "from nemo_curator import ScoreFilter\n",
    "\n",
    "# Define paths and load the dataset\n",
    "CF_input_data_dir = HF_output_path\n",
    "CF_output_path = os.path.join(data_dir, 'classifier_filtering/output')\n",
    "target_dataset = DocumentDataset.read_parquet(CF_input_data_dir, 'parquet')\n",
    "\n",
    "# Set up the filtering pipeline\n",
    "filter_pipeline = ScoreFilter(FastTextQualityFilter(model_path), score_field=\"quality_score\", score_type=float)\n",
    "filtered_dataset = filter_pipeline(target_dataset)\n",
    "\n",
    "# Save the filtered dataset\n",
    "write_to_disk(filtered_dataset.df, output_file_dir=CF_output_path, write_to_filename=True, output_type='parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the CPU Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the notebook! For other techniques such as Fuzzy Deduplication or PII redaction, you can go to [NeMo Curator example scripts](https://github.com/NVIDIA/NeMo-Curator/tree/main/examples)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
